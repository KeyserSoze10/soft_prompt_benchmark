
CondaSystemExit: Exiting.



==> WARNING: A newer version of conda exists. <==
  current version: 4.11.0
  latest version: 4.12.0

Please update conda by running

    $ conda update -n base -c defaults conda


Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[INFO|tokenization_auto.py:334] 2022-04-19 01:28:03,648 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:583] 2022-04-19 01:28:03,784 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/ask9126/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff
[INFO|configuration_utils.py:620] 2022-04-19 01:28:03,785 >> Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.11.3",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:1741] 2022-04-19 01:28:04,774 >> loading file https://huggingface.co/gpt2-medium/resolve/main/vocab.json from cache at /home/ask9126/.cache/huggingface/transformers/fee58641d7a73348d842afaa337d5a7763dad32beff8d9008bb3c3c847749d6b.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
[INFO|tokenization_utils_base.py:1741] 2022-04-19 01:28:04,774 >> loading file https://huggingface.co/gpt2-medium/resolve/main/merges.txt from cache at /home/ask9126/.cache/huggingface/transformers/23c853a0fcfc12c7d72ad4e922068b6982665b673f6de30b4c5cbe5bd70a2236.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1741] 2022-04-19 01:28:04,774 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer.json from cache at /home/ask9126/.cache/huggingface/transformers/8e4f9a65085b1b4ae69ffac9a953a44249c9ea1e72e4a7816ee87b70081df038.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
[INFO|tokenization_utils_base.py:1741] 2022-04-19 01:28:04,774 >> loading file https://huggingface.co/gpt2-medium/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-04-19 01:28:04,774 >> loading file https://huggingface.co/gpt2-medium/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1741] 2022-04-19 01:28:04,774 >> loading file https://huggingface.co/gpt2-medium/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:583] 2022-04-19 01:28:04,906 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/ask9126/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff
[INFO|configuration_utils.py:620] 2022-04-19 01:28:04,907 >> Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.11.3",
  "use_cache": true,
  "vocab_size": 50257
}

  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 39.17it/s]
Running tokenizer on dataset:   0%|          | 0/1 [00:00<?, ?ba/s]Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  9.38ba/s]Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  9.36ba/s]
[INFO|configuration_utils.py:583] 2022-04-19 01:28:06,577 >> loading configuration file https://huggingface.co/gpt2-medium/resolve/main/config.json from cache at /home/ask9126/.cache/huggingface/transformers/3a7a4b7235202f93d14a4a5e8200709184c5b25a29d9cfa6b0ede5166adf0768.cf0ec4a33a38dc96108560e01338af4bd3360dd859385d451c35b41987ae73ff
[INFO|configuration_utils.py:620] 2022-04-19 01:28:06,578 >> Model config GPT2Config {
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "finetuning_task": "rte",
  "id2label": {
    "0": "entailment",
    "1": "not_entailment"
  },
  "initializer_range": 0.02,
  "label2id": {
    "entailment": 0,
    "not_entailment": 1
  },
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1024,
  "n_head": 16,
  "n_inner": null,
  "n_layer": 24,
  "n_positions": 1024,
  "n_special": 0,
  "predict_special_tokens": true,
  "resid_pdrop": 0.1,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.11.3",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|file_utils.py:1664] 2022-04-19 01:28:09,230 >> https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ask9126/.cache/huggingface/transformers/tmpcw3u6efd
Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]Downloading:   0%|          | 1.47M/1.42G [00:00<01:38, 15.4MB/s]Downloading:   0%|          | 6.60M/1.42G [00:00<00:39, 37.9MB/s]Downloading:   1%|          | 10.2M/1.42G [00:00<00:53, 28.0MB/s]Downloading:   1%|          | 15.1M/1.42G [00:00<00:43, 34.2MB/s]Downloading:   2%|▏         | 22.1M/1.42G [00:00<00:32, 46.1MB/s]Downloading:   2%|▏         | 26.8M/1.42G [00:00<00:32, 46.4MB/s]Downloading:   3%|▎         | 36.4M/1.42G [00:00<00:23, 62.5MB/s]Downloading:   3%|▎         | 42.6M/1.42G [00:01<00:34, 42.8MB/s]Downloading:   3%|▎         | 49.3M/1.42G [00:01<00:29, 49.0MB/s]Downloading:   4%|▍         | 54.9M/1.42G [00:01<00:30, 47.7MB/s]Downloading:   4%|▍         | 63.6M/1.42G [00:01<00:24, 58.3MB/s]Downloading:   5%|▌         | 73.2M/1.42G [00:01<00:20, 69.5MB/s]Downloading:   6%|▌         | 83.0M/1.42G [00:01<00:18, 78.3MB/s]Downloading:   6%|▋         | 92.9M/1.42G [00:01<00:16, 85.3MB/s]Downloading:   7%|▋         | 102M/1.42G [00:01<00:15, 88.9MB/s] Downloading:   8%|▊         | 111M/1.42G [00:01<00:15, 88.0MB/s]Downloading:   8%|▊         | 120M/1.42G [00:02<00:18, 76.3MB/s]Downloading:   9%|▉         | 127M/1.42G [00:02<00:19, 70.8MB/s]Downloading:   9%|▉         | 134M/1.42G [00:02<00:23, 58.1MB/s]Downloading:  10%|▉         | 144M/1.42G [00:02<00:20, 68.0MB/s]Downloading:  11%|█         | 154M/1.42G [00:02<00:17, 76.0MB/s]Downloading:  11%|█▏        | 163M/1.42G [00:02<00:16, 82.8MB/s]Downloading:  12%|█▏        | 173M/1.42G [00:02<00:15, 87.8MB/s]Downloading:  13%|█▎        | 182M/1.42G [00:02<00:17, 75.8MB/s]Downloading:  13%|█▎        | 192M/1.42G [00:03<00:16, 82.1MB/s]Downloading:  14%|█▍        | 201M/1.42G [00:03<00:15, 87.2MB/s]Downloading:  15%|█▍        | 211M/1.42G [00:03<00:14, 91.6MB/s]Downloading:  15%|█▌        | 221M/1.42G [00:03<00:13, 94.7MB/s]Downloading:  16%|█▌        | 231M/1.42G [00:03<00:13, 96.8MB/s]Downloading:  17%|█▋        | 240M/1.42G [00:03<00:12, 98.2MB/s]Downloading:  17%|█▋        | 250M/1.42G [00:03<00:12, 98.5MB/s]Downloading:  18%|█▊        | 259M/1.42G [00:03<00:12, 99.5MB/s]Downloading:  19%|█▊        | 269M/1.42G [00:03<00:12, 100MB/s] Downloading:  19%|█▉        | 279M/1.42G [00:03<00:12, 101MB/s]Downloading:  20%|█▉        | 289M/1.42G [00:04<00:14, 85.6MB/s]Downloading:  21%|██        | 298M/1.42G [00:04<00:13, 90.2MB/s]Downloading:  21%|██▏       | 308M/1.42G [00:04<00:12, 94.0MB/s]Downloading:  22%|██▏       | 318M/1.42G [00:04<00:12, 96.7MB/s]Downloading:  23%|██▎       | 328M/1.42G [00:04<00:11, 98.2MB/s]Downloading:  23%|██▎       | 338M/1.42G [00:04<00:11, 99.2MB/s]Downloading:  24%|██▍       | 348M/1.42G [00:04<00:11, 101MB/s] Downloading:  25%|██▍       | 357M/1.42G [00:04<00:11, 101MB/s]Downloading:  25%|██▌       | 367M/1.42G [00:04<00:11, 101MB/s]Downloading:  26%|██▌       | 377M/1.42G [00:05<00:11, 102MB/s]Downloading:  27%|██▋       | 387M/1.42G [00:05<00:12, 91.6MB/s]Downloading:  27%|██▋       | 395M/1.42G [00:05<00:16, 68.8MB/s]Downloading:  28%|██▊       | 405M/1.42G [00:05<00:14, 76.1MB/s]Downloading:  29%|██▊       | 413M/1.42G [00:05<00:17, 63.6MB/s]Downloading:  29%|██▉       | 421M/1.42G [00:05<00:15, 68.2MB/s]Downloading:  30%|██▉       | 431M/1.42G [00:05<00:13, 76.7MB/s]Downloading:  30%|███       | 441M/1.42G [00:05<00:12, 83.7MB/s]Downloading:  31%|███       | 451M/1.42G [00:06<00:11, 89.3MB/s]Downloading:  32%|███▏      | 461M/1.42G [00:06<00:11, 92.4MB/s]Downloading:  32%|███▏      | 470M/1.42G [00:06<00:10, 95.4MB/s]Downloading:  33%|███▎      | 480M/1.42G [00:06<00:10, 97.5MB/s]Downloading:  34%|███▍      | 490M/1.42G [00:06<00:10, 99.0MB/s]Downloading:  34%|███▍      | 500M/1.42G [00:06<00:09, 101MB/s] Downloading:  35%|███▌      | 510M/1.42G [00:06<00:09, 102MB/s]Downloading:  36%|███▌      | 520M/1.42G [00:06<00:09, 102MB/s]Downloading:  37%|███▋      | 529M/1.42G [00:06<00:09, 102MB/s]Downloading:  37%|███▋      | 539M/1.42G [00:06<00:09, 95.6MB/s]Downloading:  38%|███▊      | 548M/1.42G [00:07<00:10, 91.9MB/s]Downloading:  38%|███▊      | 557M/1.42G [00:07<00:10, 92.0MB/s]Downloading:  39%|███▉      | 567M/1.42G [00:07<00:09, 95.1MB/s]Downloading:  40%|███▉      | 577M/1.42G [00:07<00:09, 97.3MB/s]Downloading:  40%|████      | 587M/1.42G [00:07<00:09, 99.0MB/s]Downloading:  41%|████      | 596M/1.42G [00:07<00:08, 99.5MB/s]Downloading:  42%|████▏     | 606M/1.42G [00:07<00:08, 100MB/s] Downloading:  42%|████▏     | 615M/1.42G [00:07<00:09, 92.4MB/s]Downloading:  43%|████▎     | 625M/1.42G [00:07<00:09, 94.7MB/s]Downloading:  44%|████▍     | 635M/1.42G [00:08<00:08, 96.9MB/s]Downloading:  44%|████▍     | 644M/1.42G [00:08<00:10, 80.0MB/s]Downloading:  45%|████▍     | 652M/1.42G [00:08<00:11, 72.2MB/s]Downloading:  46%|████▌     | 660M/1.42G [00:08<00:11, 73.9MB/s]Downloading:  46%|████▌     | 669M/1.42G [00:08<00:10, 80.6MB/s]Downloading:  47%|████▋     | 679M/1.42G [00:08<00:09, 86.4MB/s]Downloading:  48%|████▊     | 689M/1.42G [00:08<00:08, 90.7MB/s]Downloading:  48%|████▊     | 699M/1.42G [00:08<00:08, 94.3MB/s]Downloading:  49%|████▉     | 708M/1.42G [00:08<00:08, 96.5MB/s]Downloading:  50%|████▉     | 718M/1.42G [00:09<00:07, 98.4MB/s]Downloading:  50%|█████     | 728M/1.42G [00:09<00:07, 99.8MB/s]Downloading:  51%|█████     | 738M/1.42G [00:09<00:07, 101MB/s] Downloading:  52%|█████▏    | 748M/1.42G [00:09<00:07, 101MB/s]Downloading:  52%|█████▏    | 757M/1.42G [00:09<00:07, 102MB/s]Downloading:  53%|█████▎    | 767M/1.42G [00:09<00:11, 64.3MB/s]Downloading:  54%|█████▎    | 777M/1.42G [00:09<00:09, 72.7MB/s]Downloading:  54%|█████▍    | 787M/1.42G [00:09<00:08, 79.3MB/s]Downloading:  55%|█████▍    | 795M/1.42G [00:10<00:13, 52.3MB/s]Downloading:  56%|█████▌    | 805M/1.42G [00:10<00:11, 61.2MB/s]Downloading:  56%|█████▌    | 815M/1.42G [00:10<00:09, 69.6MB/s]Downloading:  57%|█████▋    | 824M/1.42G [00:10<00:08, 76.9MB/s]Downloading:  58%|█████▊    | 834M/1.42G [00:10<00:07, 83.5MB/s]Downloading:  58%|█████▊    | 844M/1.42G [00:10<00:07, 88.6MB/s]Downloading:  59%|█████▉    | 854M/1.42G [00:10<00:06, 92.7MB/s]Downloading:  60%|█████▉    | 864M/1.42G [00:10<00:06, 95.1MB/s]Downloading:  60%|██████    | 873M/1.42G [00:11<00:06, 97.3MB/s]Downloading:  61%|██████    | 883M/1.42G [00:11<00:05, 99.0MB/s]Downloading:  62%|██████▏   | 893M/1.42G [00:11<00:07, 75.9MB/s]Downloading:  62%|██████▏   | 901M/1.42G [00:11<00:08, 68.9MB/s]Downloading:  63%|██████▎   | 908M/1.42G [00:11<00:08, 63.1MB/s]Downloading:  63%|██████▎   | 918M/1.42G [00:11<00:07, 71.7MB/s]Downloading:  64%|██████▍   | 928M/1.42G [00:11<00:06, 79.3MB/s]Downloading:  65%|██████▍   | 937M/1.42G [00:11<00:06, 84.7MB/s]Downloading:  65%|██████▌   | 947M/1.42G [00:12<00:05, 89.2MB/s]Downloading:  66%|██████▌   | 957M/1.42G [00:12<00:05, 92.1MB/s]Downloading:  67%|██████▋   | 966M/1.42G [00:12<00:05, 86.4MB/s]Downloading:  67%|██████▋   | 974M/1.42G [00:12<00:08, 59.3MB/s]Downloading:  68%|██████▊   | 984M/1.42G [00:12<00:07, 68.3MB/s]Downloading:  69%|██████▊   | 994M/1.42G [00:12<00:06, 76.1MB/s]Downloading:  69%|██████▉   | 0.98G/1.42G [00:12<00:06, 73.6MB/s]Downloading:  70%|██████▉   | 0.99G/1.42G [00:13<00:07, 63.0MB/s]Downloading:  70%|███████   | 1.00G/1.42G [00:13<00:06, 71.4MB/s]Downloading:  71%|███████   | 1.00G/1.42G [00:13<00:05, 77.8MB/s]Downloading:  72%|███████▏  | 1.01G/1.42G [00:13<00:05, 82.6MB/s]Downloading:  72%|███████▏  | 1.02G/1.42G [00:13<00:04, 88.3MB/s]Downloading:  73%|███████▎  | 1.03G/1.42G [00:13<00:04, 91.6MB/s]Downloading:  74%|███████▎  | 1.04G/1.42G [00:13<00:04, 94.5MB/s]Downloading:  74%|███████▍  | 1.05G/1.42G [00:13<00:04, 97.1MB/s]Downloading:  75%|███████▍  | 1.06G/1.42G [00:13<00:03, 99.0MB/s]Downloading:  76%|███████▌  | 1.07G/1.42G [00:14<00:03, 100MB/s] Downloading:  76%|███████▋  | 1.08G/1.42G [00:14<00:03, 101MB/s]Downloading:  77%|███████▋  | 1.09G/1.42G [00:14<00:03, 102MB/s]Downloading:  78%|███████▊  | 1.10G/1.42G [00:14<00:03, 102MB/s]Downloading:  78%|███████▊  | 1.11G/1.42G [00:14<00:04, 66.2MB/s]Downloading:  79%|███████▉  | 1.12G/1.42G [00:14<00:05, 57.8MB/s]Downloading:  80%|███████▉  | 1.13G/1.42G [00:14<00:04, 66.3MB/s]Downloading:  80%|████████  | 1.14G/1.42G [00:14<00:04, 74.1MB/s]Downloading:  81%|████████  | 1.15G/1.42G [00:15<00:03, 81.0MB/s]Downloading:  82%|████████▏ | 1.15G/1.42G [00:15<00:03, 86.4MB/s]Downloading:  82%|████████▏ | 1.16G/1.42G [00:15<00:02, 91.0MB/s]Downloading:  83%|████████▎ | 1.17G/1.42G [00:15<00:02, 94.2MB/s]Downloading:  84%|████████▎ | 1.18G/1.42G [00:15<00:02, 96.8MB/s]Downloading:  84%|████████▍ | 1.19G/1.42G [00:15<00:02, 97.3MB/s]Downloading:  85%|████████▍ | 1.20G/1.42G [00:15<00:02, 97.9MB/s]Downloading:  86%|████████▌ | 1.21G/1.42G [00:15<00:02, 99.2MB/s]Downloading:  86%|████████▋ | 1.22G/1.42G [00:15<00:02, 100MB/s] Downloading:  87%|████████▋ | 1.23G/1.42G [00:15<00:01, 100MB/s]Downloading:  88%|████████▊ | 1.24G/1.42G [00:16<00:02, 88.7MB/s]Downloading:  88%|████████▊ | 1.25G/1.42G [00:16<00:03, 56.9MB/s]Downloading:  89%|████████▊ | 1.26G/1.42G [00:16<00:02, 60.7MB/s]Downloading:  89%|████████▉ | 1.26G/1.42G [00:16<00:02, 68.6MB/s]Downloading:  90%|████████▉ | 1.27G/1.42G [00:16<00:02, 72.1MB/s]Downloading:  91%|█████████ | 1.28G/1.42G [00:16<00:01, 79.6MB/s]Downloading:  91%|█████████ | 1.29G/1.42G [00:16<00:01, 85.5MB/s]Downloading:  92%|█████████▏| 1.30G/1.42G [00:17<00:01, 89.7MB/s]Downloading:  93%|█████████▎| 1.31G/1.42G [00:17<00:01, 93.0MB/s]Downloading:  93%|█████████▎| 1.32G/1.42G [00:17<00:01, 95.5MB/s]Downloading:  94%|█████████▍| 1.33G/1.42G [00:17<00:00, 97.4MB/s]Downloading:  95%|█████████▍| 1.34G/1.42G [00:17<00:00, 98.9MB/s]Downloading:  95%|█████████▌| 1.35G/1.42G [00:17<00:00, 99.8MB/s]Downloading:  96%|█████████▌| 1.36G/1.42G [00:17<00:00, 100MB/s] Downloading:  97%|█████████▋| 1.37G/1.42G [00:17<00:00, 68.3MB/s]Downloading:  97%|█████████▋| 1.37G/1.42G [00:18<00:00, 52.4MB/s]Downloading:  98%|█████████▊| 1.38G/1.42G [00:18<00:00, 61.5MB/s]Downloading:  98%|█████████▊| 1.39G/1.42G [00:18<00:00, 70.2MB/s]Downloading:  99%|█████████▉| 1.40G/1.42G [00:18<00:00, 77.8MB/s]Downloading: 100%|█████████▉| 1.41G/1.42G [00:18<00:00, 83.8MB/s]Downloading: 100%|██████████| 1.42G/1.42G [00:18<00:00, 81.8MB/s]
[INFO|file_utils.py:1668] 2022-04-19 01:28:33,581 >> storing https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin in cache at /home/ask9126/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1
[INFO|file_utils.py:1676] 2022-04-19 01:28:33,588 >> creating metadata file for /home/ask9126/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1
[INFO|modeling_utils.py:1323] 2022-04-19 01:28:33,602 >> loading weights file https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin from cache at /home/ask9126/.cache/huggingface/transformers/6249eef5c8c1fcfccf9f36fc2e59301b109ac4036d8ebbee9c2b7f7e47f440bd.2538e2565f9e439a3668b981faf959c8b490b36dd631f3c4cd992519b2dd36f1
[WARNING|modeling_utils.py:1579] 2022-04-19 01:28:49,876 >> Some weights of the model checkpoint at gpt2-medium were not used when initializing GPT2BasePrefixForSequenceClassification: ['h.20.attn.c_attn.bias', 'h.8.ln_2.weight', 'h.23.attn.c_attn.weight', 'h.2.mlp.c_proj.bias', 'h.8.mlp.c_fc.bias', 'h.0.attn.c_attn.weight', 'h.4.mlp.c_proj.bias', 'h.23.attn.c_proj.bias', 'h.1.ln_1.bias', 'h.10.mlp.c_proj.weight', 'h.13.mlp.c_fc.weight', 'h.4.attn.bias', 'h.12.mlp.c_proj.weight', 'h.4.ln_2.bias', 'h.22.attn.c_attn.bias', 'h.12.attn.bias', 'h.4.attn.c_proj.bias', 'h.14.attn.bias', 'h.11.ln_1.weight', 'h.12.ln_1.bias', 'h.2.attn.bias', 'h.8.mlp.c_proj.weight', 'h.8.ln_1.bias', 'h.10.attn.c_attn.bias', 'h.14.mlp.c_proj.weight', 'h.11.attn.c_attn.bias', 'h.11.attn.c_proj.weight', 'h.23.mlp.c_fc.weight', 'h.1.mlp.c_fc.weight', 'h.10.ln_2.bias', 'h.14.mlp.c_fc.weight', 'h.9.ln_1.bias', 'h.18.attn.c_attn.weight', 'h.0.mlp.c_proj.bias', 'h.8.ln_2.bias', 'h.18.mlp.c_fc.bias', 'h.16.ln_1.weight', 'h.4.ln_1.weight', 'h.1.attn.c_attn.bias', 'h.9.attn.bias', 'h.6.attn.c_proj.weight', 'h.8.attn.bias', 'h.23.ln_2.bias', 'h.20.ln_1.bias', 'h.14.attn.c_attn.bias', 'h.2.mlp.c_fc.weight', 'h.22.attn.bias', 'h.2.ln_2.bias', 'h.21.ln_1.weight', 'h.0.attn.c_proj.bias', 'h.19.attn.c_proj.weight', 'h.1.ln_2.weight', 'h.20.ln_2.weight', 'h.0.attn.c_proj.weight', 'h.16.mlp.c_fc.bias', 'h.5.mlp.c_fc.weight', 'h.7.ln_2.weight', 'h.6.attn.bias', 'h.14.ln_1.bias', 'h.18.attn.c_proj.bias', 'h.5.mlp.c_proj.bias', 'h.7.ln_2.bias', 'h.19.ln_1.bias', 'h.3.ln_1.bias', 'h.5.attn.c_proj.weight', 'h.6.attn.c_attn.bias', 'h.22.ln_2.bias', 'h.5.ln_1.weight', 'h.2.mlp.c_proj.weight', 'h.12.mlp.c_fc.weight', 'h.6.mlp.c_fc.bias', 'h.14.mlp.c_proj.bias', 'h.11.mlp.c_fc.bias', 'h.7.mlp.c_fc.weight', 'h.17.attn.c_attn.bias', 'h.10.ln_2.weight', 'h.1.attn.bias', 'h.1.attn.c_proj.weight', 'h.7.ln_1.weight', 'h.2.ln_1.weight', 'h.3.attn.c_proj.weight', 'h.10.attn.c_proj.bias', 'h.17.attn.bias', 'h.22.mlp.c_fc.bias', 'h.13.ln_1.bias', 'h.19.ln_2.weight', 'h.3.ln_1.weight', 'h.0.ln_1.weight', 'h.11.ln_2.weight', 'h.4.ln_2.weight', 'h.11.attn.c_proj.bias', 'wte.weight', 'h.4.attn.c_proj.weight', 'h.13.mlp.c_proj.bias', 'h.7.mlp.c_fc.bias', 'h.21.ln_2.weight', 'h.18.ln_1.weight', 'h.23.mlp.c_fc.bias', 'h.11.mlp.c_proj.weight', 'h.18.ln_2.weight', 'h.17.attn.c_attn.weight', 'h.13.mlp.c_proj.weight', 'h.20.attn.c_attn.weight', 'h.22.attn.c_proj.bias', 'h.2.mlp.c_fc.bias', 'h.20.attn.c_proj.weight', 'h.8.attn.c_attn.weight', 'h.18.ln_2.bias', 'h.20.ln_1.weight', 'h.6.attn.c_attn.weight', 'h.15.mlp.c_fc.bias', 'h.0.mlp.c_fc.bias', 'h.21.mlp.c_proj.bias', 'h.23.mlp.c_proj.bias', 'h.7.attn.c_attn.bias', 'h.13.attn.c_attn.bias', 'h.12.attn.c_attn.weight', 'h.3.mlp.c_proj.bias', 'h.1.mlp.c_proj.bias', 'h.22.mlp.c_fc.weight', 'h.12.ln_2.weight', 'h.16.attn.c_proj.bias', 'h.19.mlp.c_proj.weight', 'h.12.ln_1.weight', 'h.11.ln_1.bias', 'h.15.ln_1.weight', 'h.23.ln_2.weight', 'h.15.attn.c_attn.bias', 'h.16.attn.bias', 'h.9.attn.c_attn.bias', 'h.1.mlp.c_proj.weight', 'h.8.ln_1.weight', 'h.16.ln_1.bias', 'h.0.ln_2.bias', 'h.11.attn.bias', 'wpe.weight', 'h.1.attn.c_proj.bias', 'h.22.ln_1.bias', 'h.16.attn.c_attn.bias', 'h.6.attn.c_proj.bias', 'h.22.mlp.c_proj.bias', 'h.7.attn.c_attn.weight', 'h.14.attn.c_proj.bias', 'h.9.attn.c_attn.weight', 'h.11.ln_2.bias', 'h.20.ln_2.bias', 'h.4.attn.c_attn.bias', 'h.5.attn.c_proj.bias', 'h.14.attn.c_attn.weight', 'h.21.attn.c_attn.weight', 'h.10.attn.c_attn.weight', 'h.15.attn.c_proj.weight', 'h.17.mlp.c_proj.bias', 'h.19.mlp.c_proj.bias', 'h.21.mlp.c_proj.weight', 'h.3.mlp.c_fc.weight', 'h.18.ln_1.bias', 'h.6.ln_1.weight', 'h.22.ln_2.weight', 'h.19.attn.c_attn.bias', 'h.15.ln_2.weight', 'h.15.mlp.c_proj.bias', 'h.2.attn.c_proj.weight', 'h.5.attn.c_attn.bias', 'h.14.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.1.mlp.c_fc.bias', 'h.12.attn.c_proj.bias', 'h.19.ln_1.weight', 'h.16.ln_2.bias', 'h.0.attn.bias', 'h.13.ln_1.weight', 'h.10.mlp.c_fc.bias', 'h.23.ln_1.weight', 'h.5.ln_2.bias', 'h.15.attn.c_proj.bias', 'h.22.attn.c_attn.weight', 'h.8.attn.c_proj.weight', 'h.4.mlp.c_fc.bias', 'h.21.mlp.c_fc.bias', 'h.13.mlp.c_fc.bias', 'h.21.ln_1.bias', 'h.20.mlp.c_fc.weight', 'h.20.attn.c_proj.bias', 'h.21.attn.c_proj.weight', 'h.23.attn.c_proj.weight', 'h.15.ln_2.bias', 'h.20.mlp.c_proj.bias', 'h.10.attn.c_proj.weight', 'h.5.ln_2.weight', 'h.17.attn.c_proj.weight', 'h.21.ln_2.bias', 'h.14.mlp.c_fc.bias', 'h.10.mlp.c_fc.weight', 'h.21.attn.bias', 'h.15.ln_1.bias', 'h.20.attn.bias', 'h.2.attn.c_attn.bias', 'h.13.attn.c_proj.weight', 'h.7.attn.bias', 'h.3.attn.c_proj.bias', 'h.6.ln_2.bias', 'h.9.mlp.c_fc.bias', 'h.8.attn.c_proj.bias', 'h.11.mlp.c_proj.bias', 'h.2.attn.c_attn.weight', 'h.13.attn.bias', 'h.5.ln_1.bias', 'h.21.mlp.c_fc.weight', 'h.9.ln_2.bias', 'h.22.ln_1.weight', 'h.21.attn.c_proj.bias', 'h.3.mlp.c_proj.weight', 'h.12.mlp.c_fc.bias', 'h.16.attn.c_attn.weight', 'h.9.mlp.c_proj.bias', 'ln_f.bias', 'h.17.mlp.c_proj.weight', 'h.3.attn.c_attn.weight', 'h.0.attn.c_attn.bias', 'h.19.ln_2.bias', 'h.8.mlp.c_fc.weight', 'h.17.ln_2.bias', 'h.3.ln_2.weight', 'h.9.ln_1.weight', 'h.23.attn.c_attn.bias', 'h.10.ln_1.bias', 'h.15.attn.bias', 'h.9.mlp.c_proj.weight', 'h.1.ln_2.bias', 'h.7.mlp.c_proj.weight', 'h.18.attn.c_attn.bias', 'h.9.mlp.c_fc.weight', 'h.10.attn.bias', 'h.5.mlp.c_proj.weight', 'h.20.mlp.c_fc.bias', 'h.6.mlp.c_proj.bias', 'h.23.attn.bias', 'h.4.attn.c_attn.weight', 'h.6.mlp.c_proj.weight', 'h.10.mlp.c_proj.bias', 'h.2.ln_2.weight', 'h.3.attn.c_attn.bias', 'h.19.mlp.c_fc.weight', 'h.19.attn.bias', 'h.10.ln_1.weight', 'h.6.ln_2.weight', 'h.9.ln_2.weight', 'h.7.attn.c_proj.weight', 'h.14.ln_2.bias', 'h.12.ln_2.bias', 'h.16.ln_2.weight', 'h.5.attn.bias', 'h.17.attn.c_proj.bias', 'h.17.mlp.c_fc.bias', 'h.18.mlp.c_proj.weight', 'h.15.mlp.c_proj.weight', 'h.22.mlp.c_proj.weight', 'h.11.mlp.c_fc.weight', 'h.19.mlp.c_fc.bias', 'h.15.mlp.c_fc.weight', 'h.18.attn.c_proj.weight', 'h.7.attn.c_proj.bias', 'h.6.ln_1.bias', 'h.22.attn.c_proj.weight', 'h.8.mlp.c_proj.bias', 'h.16.mlp.c_proj.weight', 'h.0.mlp.c_proj.weight', 'h.13.ln_2.bias', 'h.9.attn.c_proj.bias', 'h.14.ln_1.weight', 'h.17.ln_1.bias', 'h.18.attn.bias', 'h.5.mlp.c_fc.bias', 'h.17.ln_2.weight', 'h.0.mlp.c_fc.weight', 'h.3.attn.bias', 'h.19.attn.c_attn.weight', 'h.13.attn.c_proj.bias', 'ln_f.weight', 'h.12.attn.c_attn.bias', 'h.18.mlp.c_fc.weight', 'h.17.mlp.c_fc.weight', 'h.12.mlp.c_proj.bias', 'h.5.attn.c_attn.weight', 'h.8.attn.c_attn.bias', 'h.0.ln_2.weight', 'h.6.mlp.c_fc.weight', 'h.20.mlp.c_proj.weight', 'h.2.ln_1.bias', 'h.23.mlp.c_proj.weight', 'h.19.attn.c_proj.bias', 'h.1.ln_1.weight', 'h.4.ln_1.bias', 'h.3.ln_2.bias', 'h.14.ln_2.weight', 'h.16.mlp.c_proj.bias', 'h.17.ln_1.weight', 'h.11.attn.c_attn.weight', 'h.2.attn.c_proj.bias', 'h.3.mlp.c_fc.bias', 'h.7.ln_1.bias', 'h.9.attn.c_proj.weight', 'h.21.attn.c_attn.bias', 'h.12.attn.c_proj.weight', 'h.18.mlp.c_proj.bias', 'h.23.ln_1.bias', 'h.13.attn.c_attn.weight', 'h.16.mlp.c_fc.weight', 'h.7.mlp.c_proj.bias', 'h.16.attn.c_proj.weight', 'h.4.mlp.c_proj.weight', 'h.13.ln_2.weight', 'h.4.mlp.c_fc.weight', 'h.1.attn.c_attn.weight', 'h.15.attn.c_attn.weight']
- This IS expected if you are initializing GPT2BasePrefixForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing GPT2BasePrefixForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1590] 2022-04-19 01:28:49,877 >> Some weights of GPT2BasePrefixForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['gpt2.transformer.h.17.mlp.c_fc.bias', 'gpt2.transformer.h.21.mlp.c_fc.weight', 'gpt2.transformer.h.17.attn.bias', 'gpt2.transformer.h.18.attn.c_proj.weight', 'gpt2.transformer.h.0.ln_2.weight', 'gpt2.transformer.h.23.attn.c_proj.weight', 'gpt2.transformer.h.6.attn.c_attn.weight', 'gpt2.transformer.h.10.attn.bias', 'gpt2.transformer.h.14.mlp.c_fc.weight', 'gpt2.transformer.h.20.attn.c_proj.weight', 'gpt2.transformer.h.15.attn.bias', 'gpt2.transformer.h.0.mlp.c_proj.weight', 'gpt2.transformer.h.20.attn.c_proj.bias', 'gpt2.transformer.h.17.attn.c_attn.weight', 'gpt2.transformer.h.15.mlp.c_proj.weight', 'gpt2.transformer.h.20.attn.c_attn.bias', 'gpt2.transformer.h.23.attn.masked_bias', 'gpt2.transformer.h.12.attn.masked_bias', 'gpt2.transformer.h.9.attn.c_attn.bias', 'gpt2.transformer.h.8.ln_1.bias', 'gpt2.transformer.h.3.ln_2.bias', 'gpt2.transformer.h.6.ln_2.weight', 'gpt2.transformer.h.5.attn.c_proj.weight', 'gpt2.transformer.h.16.attn.c_attn.weight', 'gpt2.transformer.h.19.attn.c_proj.weight', 'gpt2.transformer.h.0.mlp.c_fc.weight', 'gpt2.transformer.h.5.ln_2.bias', 'gpt2.transformer.h.17.ln_2.bias', 'gpt2.transformer.h.10.attn.c_attn.weight', 'gpt2.transformer.h.2.mlp.c_fc.bias', 'gpt2.transformer.h.17.attn.masked_bias', 'gpt2.transformer.wte.weight', 'gpt2.transformer.h.13.ln_1.weight', 'gpt2.transformer.h.15.ln_1.weight', 'gpt2.transformer.h.13.mlp.c_proj.bias', 'gpt2.transformer.h.9.ln_1.weight', 'gpt2.transformer.h.12.ln_2.bias', 'gpt2.transformer.h.10.mlp.c_proj.bias', 'gpt2.transformer.h.1.attn.c_attn.bias', 'gpt2.transformer.h.12.attn.bias', 'gpt2.transformer.h.4.mlp.c_fc.weight', 'gpt2.transformer.h.11.attn.masked_bias', 'gpt2.transformer.h.7.attn.c_proj.bias', 'gpt2.transformer.h.11.mlp.c_proj.bias', 'gpt2.transformer.h.23.mlp.c_proj.bias', 'gpt2.transformer.h.18.ln_1.weight', 'gpt2.transformer.h.9.attn.c_proj.weight', 'gpt2.transformer.ln_f.weight', 'gpt2.transformer.h.22.attn.masked_bias', 'gpt2.transformer.h.17.ln_1.weight', 'gpt2.transformer.h.12.mlp.c_proj.bias', 'gpt2.transformer.wpe.weight', 'gpt2.transformer.h.11.attn.c_proj.weight', 'gpt2.transformer.h.1.mlp.c_fc.bias', 'gpt2.transformer.h.20.mlp.c_proj.bias', 'gpt2.transformer.h.11.attn.c_attn.bias', 'gpt2.transformer.h.13.attn.c_attn.weight', 'gpt2.transformer.h.2.ln_1.weight', 'gpt2.transformer.h.4.mlp.c_fc.bias', 'gpt2.transformer.h.6.ln_1.bias', 'gpt2.transformer.h.16.ln_2.bias', 'gpt2.transformer.h.2.ln_1.bias', 'gpt2.transformer.h.14.ln_2.bias', 'gpt2.transformer.h.7.attn.c_attn.bias', 'gpt2.transformer.h.23.ln_2.weight', 'gpt2.transformer.h.0.attn.c_attn.weight', 'gpt2.transformer.h.0.attn.masked_bias', 'gpt2.transformer.h.7.attn.masked_bias', 'gpt2.transformer.h.9.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_2.weight', 'gpt2.transformer.h.14.attn.c_attn.bias', 'gpt2.transformer.h.21.attn.c_attn.bias', 'gpt2.transformer.h.16.ln_1.weight', 'gpt2.transformer.h.22.mlp.c_fc.weight', 'gpt2.transformer.h.18.ln_2.weight', 'gpt2.transformer.h.12.ln_2.weight', 'gpt2.transformer.h.19.mlp.c_fc.bias', 'gpt2.transformer.h.7.ln_2.bias', 'gpt2.transformer.h.19.attn.c_attn.weight', 'gpt2.transformer.h.1.mlp.c_proj.bias', 'gpt2.transformer.h.16.attn.masked_bias', 'gpt2.transformer.h.22.attn.c_attn.bias', 'gpt2.transformer.h.4.ln_1.weight', 'gpt2.transformer.h.8.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.weight', 'gpt2.transformer.h.23.attn.c_attn.weight', 'gpt2.transformer.h.19.attn.c_proj.bias', 'gpt2.transformer.h.4.attn.masked_bias', 'gpt2.transformer.h.8.attn.c_attn.weight', 'gpt2.transformer.h.0.ln_1.weight', 'gpt2.transformer.h.8.ln_1.weight', 'gpt2.transformer.h.17.mlp.c_proj.bias', 'gpt2.transformer.h.5.ln_1.weight', 'gpt2.transformer.h.22.ln_1.weight', 'gpt2.transformer.h.15.ln_2.weight', 'gpt2.transformer.h.21.mlp.c_proj.bias', 'gpt2.transformer.h.18.attn.c_attn.bias', 'gpt2.transformer.h.17.attn.c_proj.bias', 'gpt2.transformer.h.15.attn.c_attn.weight', 'gpt2.transformer.h.4.mlp.c_proj.weight', 'gpt2.transformer.h.14.attn.c_proj.bias', 'gpt2.transformer.h.2.ln_2.weight', 'gpt2.transformer.h.0.attn.c_attn.bias', 'gpt2.transformer.h.21.ln_2.bias', 'gpt2.transformer.h.7.mlp.c_proj.bias', 'gpt2.transformer.h.0.mlp.c_fc.bias', 'gpt2.transformer.h.11.attn.c_attn.weight', 'gpt2.transformer.h.10.ln_1.bias', 'gpt2.transformer.h.2.attn.c_attn.bias', 'gpt2.transformer.h.21.mlp.c_fc.bias', 'gpt2.transformer.h.0.attn.c_proj.weight', 'gpt2.transformer.h.7.attn.c_proj.weight', 'gpt2.transformer.h.13.attn.c_attn.bias', 'gpt2.transformer.h.22.attn.c_proj.bias', 'gpt2.transformer.h.3.ln_1.weight', 'gpt2.transformer.h.5.mlp.c_fc.weight', 'gpt2.transformer.ln_f.bias', 'gpt2.transformer.h.5.attn.c_proj.bias', 'gpt2.transformer.h.11.ln_2.bias', 'gpt2.transformer.h.6.attn.c_proj.weight', 'gpt2.transformer.h.9.ln_2.weight', 'gpt2.transformer.h.3.ln_2.weight', 'gpt2.transformer.h.22.attn.bias', 'gpt2.transformer.h.11.ln_2.weight', 'gpt2.transformer.h.8.attn.c_attn.bias', 'gpt2.transformer.h.6.ln_2.bias', 'prefix_encoder.embedding.weight', 'gpt2.transformer.h.4.attn.bias', 'gpt2.transformer.h.5.attn.bias', 'gpt2.transformer.h.11.attn.bias', 'gpt2.transformer.h.23.attn.bias', 'gpt2.transformer.h.10.attn.c_proj.bias', 'gpt2.transformer.h.14.mlp.c_proj.bias', 'gpt2.transformer.h.3.attn.bias', 'gpt2.transformer.h.22.attn.c_attn.weight', 'gpt2.transformer.h.5.attn.masked_bias', 'gpt2.transformer.h.18.mlp.c_fc.weight', 'gpt2.transformer.h.12.attn.c_proj.bias', 'gpt2.transformer.h.17.mlp.c_fc.weight', 'gpt2.transformer.h.21.attn.c_proj.weight', 'gpt2.transformer.h.16.mlp.c_fc.weight', 'gpt2.transformer.h.2.ln_2.bias', 'gpt2.transformer.h.22.attn.c_proj.weight', 'gpt2.transformer.h.22.mlp.c_proj.weight', 'gpt2.transformer.h.7.attn.c_attn.weight', 'gpt2.transformer.h.8.ln_2.bias', 'gpt2.transformer.h.13.mlp.c_proj.weight', 'gpt2.transformer.h.10.ln_2.weight', 'gpt2.transformer.h.18.ln_2.bias', 'gpt2.transformer.h.14.attn.bias', 'gpt2.transformer.h.23.attn.c_proj.bias', 'gpt2.transformer.h.3.mlp.c_proj.bias', 'gpt2.transformer.h.9.mlp.c_proj.weight', 'gpt2.transformer.h.1.attn.c_proj.weight', 'gpt2.transformer.h.4.attn.c_proj.bias', 'gpt2.transformer.h.6.attn.c_proj.bias', 'gpt2.transformer.h.18.mlp.c_proj.bias', 'gpt2.transformer.h.19.attn.c_attn.bias', 'gpt2.transformer.h.6.attn.bias', 'gpt2.transformer.h.14.ln_1.bias', 'gpt2.transformer.h.8.mlp.c_proj.bias', 'gpt2.transformer.h.21.ln_1.weight', 'gpt2.transformer.h.9.ln_2.bias', 'gpt2.transformer.h.23.mlp.c_fc.weight', 'gpt2.transformer.h.8.attn.bias', 'gpt2.transformer.h.16.ln_1.bias', 'gpt2.transformer.h.16.mlp.c_proj.weight', 'gpt2.transformer.h.2.mlp.c_proj.weight', 'gpt2.transformer.h.14.attn.c_attn.weight', 'gpt2.transformer.h.8.attn.c_proj.weight', 'gpt2.transformer.h.11.ln_1.weight', 'gpt2.transformer.h.3.attn.c_attn.weight', 'gpt2.transformer.h.4.ln_1.bias', 'gpt2.transformer.h.9.attn.c_proj.bias', 'gpt2.transformer.h.2.mlp.c_proj.bias', 'gpt2.transformer.h.15.ln_2.bias', 'gpt2.transformer.h.10.mlp.c_fc.weight', 'gpt2.transformer.h.12.ln_1.weight', 'gpt2.transformer.h.3.mlp.c_fc.weight', 'gpt2.transformer.h.2.attn.c_proj.bias', 'gpt2.transformer.h.6.mlp.c_fc.weight', 'gpt2.transformer.h.13.attn.bias', 'gpt2.transformer.h.23.ln_1.bias', 'gpt2.transformer.h.4.attn.c_attn.weight', 'gpt2.transformer.h.8.mlp.c_fc.weight', 'gpt2.transformer.h.9.mlp.c_fc.weight', 'gpt2.transformer.h.17.mlp.c_proj.weight', 'gpt2.transformer.h.7.attn.bias', 'gpt2.transformer.h.14.mlp.c_proj.weight', 'gpt2.transformer.h.15.attn.c_proj.weight', 'gpt2.transformer.h.22.ln_2.bias', 'gpt2.transformer.h.10.attn.c_attn.bias', 'gpt2.transformer.h.14.attn.masked_bias', 'gpt2.transformer.h.21.attn.c_proj.bias', 'gpt2.transformer.h.1.attn.bias', 'gpt2.transformer.h.23.mlp.c_proj.weight', 'gpt2.transformer.h.16.attn.bias', 'gpt2.transformer.h.11.mlp.c_proj.weight', 'gpt2.transformer.h.20.attn.c_attn.weight', 'gpt2.transformer.h.6.mlp.c_proj.weight', 'gpt2.transformer.h.15.attn.c_proj.bias', 'gpt2.transformer.h.6.mlp.c_proj.bias', 'gpt2.transformer.h.20.mlp.c_proj.weight', 'gpt2.transformer.h.2.attn.c_proj.weight', 'gpt2.transformer.h.21.ln_1.bias', 'gpt2.transformer.h.3.attn.c_proj.bias', 'gpt2.transformer.h.6.ln_1.weight', 'gpt2.transformer.h.0.attn.c_proj.bias', 'classifier.weight', 'gpt2.transformer.h.12.attn.c_proj.weight', 'gpt2.transformer.h.21.attn.bias', 'gpt2.transformer.h.15.attn.c_attn.bias', 'gpt2.transformer.h.12.ln_1.bias', 'gpt2.transformer.h.13.mlp.c_fc.bias', 'gpt2.transformer.h.16.ln_2.weight', 'gpt2.transformer.h.6.attn.masked_bias', 'gpt2.transformer.h.16.mlp.c_fc.bias', 'gpt2.transformer.h.14.ln_2.weight', 'gpt2.transformer.h.19.ln_1.weight', 'gpt2.transformer.h.18.attn.masked_bias', 'gpt2.transformer.h.10.mlp.c_fc.bias', 'gpt2.transformer.h.23.mlp.c_fc.bias', 'gpt2.transformer.h.0.mlp.c_proj.bias', 'gpt2.transformer.h.1.ln_1.bias', 'gpt2.transformer.h.13.attn.masked_bias', 'gpt2.transformer.h.19.attn.bias', 'gpt2.transformer.h.17.attn.c_proj.weight', 'gpt2.transformer.h.19.ln_2.weight', 'gpt2.transformer.h.9.attn.c_attn.weight', 'gpt2.transformer.h.1.mlp.c_fc.weight', 'gpt2.transformer.h.12.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_proj.weight', 'gpt2.transformer.h.9.attn.bias', 'gpt2.transformer.h.13.attn.c_proj.weight', 'classifier.bias', 'gpt2.transformer.h.0.ln_2.bias', 'gpt2.transformer.h.10.ln_2.bias', 'gpt2.transformer.h.12.attn.c_attn.weight', 'gpt2.transformer.h.13.ln_2.bias', 'gpt2.transformer.h.1.ln_2.bias', 'gpt2.transformer.h.11.mlp.c_fc.weight', 'gpt2.transformer.h.7.ln_1.weight', 'gpt2.transformer.h.1.ln_2.weight', 'gpt2.transformer.h.8.ln_2.weight', 'gpt2.transformer.h.15.mlp.c_proj.bias', 'gpt2.transformer.h.17.attn.c_attn.bias', 'gpt2.transformer.h.22.ln_2.weight', 'gpt2.transformer.h.14.mlp.c_fc.bias', 'gpt2.transformer.h.4.mlp.c_proj.bias', 'gpt2.transformer.h.4.ln_2.weight', 'gpt2.transformer.h.13.ln_2.weight', 'gpt2.transformer.h.1.ln_1.weight', 'gpt2.transformer.h.19.mlp.c_proj.weight', 'gpt2.transformer.h.7.mlp.c_fc.weight', 'gpt2.transformer.h.2.attn.c_attn.weight', 'gpt2.transformer.h.20.attn.masked_bias', 'gpt2.transformer.h.15.mlp.c_fc.weight', 'gpt2.transformer.h.4.attn.c_attn.bias', 'gpt2.transformer.h.20.mlp.c_fc.weight', 'gpt2.transformer.h.22.ln_1.bias', 'gpt2.transformer.h.21.attn.masked_bias', 'gpt2.transformer.h.9.mlp.c_fc.bias', 'gpt2.transformer.h.15.attn.masked_bias', 'gpt2.transformer.h.7.mlp.c_fc.bias', 'gpt2.transformer.h.7.mlp.c_proj.weight', 'gpt2.transformer.h.19.mlp.c_fc.weight', 'gpt2.transformer.h.9.attn.masked_bias', 'gpt2.transformer.h.12.mlp.c_fc.bias', 'gpt2.transformer.h.6.mlp.c_fc.bias', 'gpt2.transformer.h.19.mlp.c_proj.bias', 'gpt2.transformer.h.20.ln_1.weight', 'gpt2.transformer.h.18.ln_1.bias', 'gpt2.transformer.h.20.ln_2.weight', 'gpt2.transformer.h.16.attn.c_proj.weight', 'gpt2.transformer.h.20.mlp.c_fc.bias', 'gpt2.transformer.h.1.mlp.c_proj.weight', 'gpt2.transformer.h.3.mlp.c_fc.bias', 'gpt2.transformer.h.0.ln_1.bias', 'gpt2.transformer.h.20.ln_1.bias', 'gpt2.transformer.h.0.attn.bias', 'gpt2.transformer.h.19.ln_1.bias', 'gpt2.transformer.h.22.mlp.c_fc.bias', 'gpt2.transformer.h.12.mlp.c_fc.weight', 'gpt2.transformer.h.23.ln_1.weight', 'gpt2.transformer.h.5.attn.c_attn.weight', 'gpt2.transformer.h.10.attn.c_proj.weight', 'gpt2.transformer.h.18.attn.bias', 'gpt2.transformer.h.18.mlp.c_proj.weight', 'gpt2.transformer.h.5.mlp.c_proj.bias', 'gpt2.transformer.h.3.ln_1.bias', 'gpt2.transformer.h.2.attn.masked_bias', 'gpt2.transformer.h.19.attn.masked_bias', 'gpt2.transformer.h.15.ln_1.bias', 'gpt2.transformer.h.2.attn.bias', 'gpt2.transformer.h.23.attn.c_attn.bias', 'gpt2.transformer.h.22.mlp.c_proj.bias', 'gpt2.transformer.h.3.attn.c_proj.weight', 'gpt2.transformer.h.16.attn.c_attn.bias', 'gpt2.transformer.h.5.attn.c_attn.bias', 'gpt2.transformer.h.21.attn.c_attn.weight', 'gpt2.transformer.h.3.attn.masked_bias', 'gpt2.transformer.h.4.attn.c_proj.weight', 'gpt2.transformer.h.18.attn.c_attn.weight', 'gpt2.transformer.h.7.ln_1.bias', 'gpt2.transformer.h.12.attn.c_attn.bias', 'gpt2.transformer.h.1.attn.masked_bias', 'gpt2.transformer.h.8.attn.c_proj.bias', 'gpt2.transformer.h.11.attn.c_proj.bias', 'gpt2.transformer.h.7.ln_2.weight', 'gpt2.transformer.h.21.ln_2.weight', 'gpt2.transformer.h.14.ln_1.weight', 'gpt2.transformer.h.5.ln_1.bias', 'gpt2.transformer.h.1.attn.c_proj.bias', 'gpt2.transformer.h.14.attn.c_proj.weight', 'gpt2.transformer.h.16.attn.c_proj.bias', 'gpt2.transformer.h.8.mlp.c_fc.bias', 'gpt2.transformer.h.10.mlp.c_proj.weight', 'gpt2.transformer.h.8.attn.masked_bias', 'gpt2.transformer.h.13.attn.c_proj.bias', 'gpt2.transformer.h.10.attn.masked_bias', 'gpt2.transformer.h.21.mlp.c_proj.weight', 'gpt2.transformer.h.19.ln_2.bias', 'gpt2.score.weight', 'gpt2.transformer.h.17.ln_2.weight', 'gpt2.transformer.h.17.ln_1.bias', 'gpt2.transformer.h.3.attn.c_attn.bias', 'gpt2.transformer.h.6.attn.c_attn.bias', 'gpt2.transformer.h.16.mlp.c_proj.bias', 'gpt2.transformer.h.23.ln_2.bias', 'gpt2.transformer.h.20.ln_2.bias', 'gpt2.transformer.h.1.attn.c_attn.weight', 'gpt2.transformer.h.11.mlp.c_fc.bias', 'gpt2.transformer.h.20.attn.bias', 'gpt2.transformer.h.18.attn.c_proj.bias', 'gpt2.transformer.h.9.ln_1.bias', 'gpt2.transformer.h.13.mlp.c_fc.weight', 'gpt2.transformer.h.10.ln_1.weight', 'gpt2.transformer.h.13.ln_1.bias', 'gpt2.transformer.h.5.mlp.c_fc.bias', 'gpt2.transformer.h.15.mlp.c_fc.bias', 'gpt2.transformer.h.18.mlp.c_fc.bias', 'gpt2.transformer.h.4.ln_2.bias', 'gpt2.transformer.h.11.ln_1.bias', 'gpt2.transformer.h.2.mlp.c_fc.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:540] 2022-04-19 01:28:59,633 >> The following columns in the training set  don't have a corresponding argument in `GPT2BasePrefixForSequenceClassification.forward` and have been ignored: premise, idx, hypothesis.
[INFO|trainer.py:1196] 2022-04-19 01:28:59,654 >> ***** Running training *****
[INFO|trainer.py:1197] 2022-04-19 01:28:59,654 >>   Num examples = 2490
[INFO|trainer.py:1198] 2022-04-19 01:28:59,654 >>   Num Epochs = 100
[INFO|trainer.py:1199] 2022-04-19 01:28:59,654 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1200] 2022-04-19 01:28:59,654 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1201] 2022-04-19 01:28:59,654 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1202] 2022-04-19 01:28:59,654 >>   Total optimization steps = 7800
  0%|          | 0/7800 [00:00<?, ?it/s]Traceback (most recent call last):
  File "run.py", line 139, in <module>
    train(trainer, training_args.resume_from_checkpoint, last_checkpoint)
  File "run.py", line 26, in train
    train_result = trainer.train(resume_from_checkpoint=checkpoint)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/trainer.py", line 1316, in train
    tr_loss_step = self.training_step(model, inputs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/trainer.py", line 1849, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/trainer.py", line 1881, in compute_loss
    outputs = model(**inputs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/scratch/ask9126/soft_prompt_benchmark/P-tuning-v2/model/sequence_classification.py", line 853, in forward
    outputs = self.gpt2(
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1281, in forward
    transformer_outputs = self.transformer(
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 798, in forward
    outputs = block(
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 318, in forward
    attn_outputs = self.attn(
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 259, in forward
    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 195, in _attn
    attn_weights = self.attn_dropout(attn_weights)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/home/ask9126/.conda/envs/pt2/lib/python3.8/site-packages/torch/nn/functional.py", line 983, in dropout
    else _VF.dropout(input, p, training))
RuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 15.78 GiB total capacity; 14.65 GiB already allocated; 20.75 MiB free; 14.65 GiB reserved in total by PyTorch)
  0%|          | 0/7800 [00:02<?, ?it/s]