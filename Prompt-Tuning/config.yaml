task_name: boolq
method: prompt_tuning
plm: t5-base

logging_steps: 50
eval_steps: 300
eval_batch_size: 512

boolq:
  prompt_tuning:
    gpt2:
      tokenizer_name: gpt2
      learning_rate: 0.01
      num_train_epochs: 100
      max_seq_length: 128
      n_prompt_tokens: 20
      init_from_vocab: true
      random_range: 0.5
    t5-base:
      tokenizer_name: t5-base
      learning_rate: 0.001
      num_train_epochs: 100
      max_seq_length: 128
      n_prompt_tokens: 20
      init_from_vocab: true
      random_range: 0.5
    roberta-base:
      tokenizer_name: roberta-base
      learning_rate: 0.001
      num_train_epochs: 100
      max_seq_length: 128
      n_prompt_tokens: 20
      init_from_vocab: true
      random_range: 0.5
